{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hierarchical Modeling\n",
    "\n",
    "This jupyter notebook accompanies the Bayesian Hierarchical Modeling lecture(s) delivered by Stephen Feeney as part of David Hogg's [Computational Data Analysis class](http://dwh.gg/FlatironCDA). As part of the lecture(s) you will be asked to complete a number of tasks, some of which will involve direct coding into the notebook; these sections are marked by task. This notebook requires numpy, matplotlib, scipy, [corner](https://github.com/sfeeney/bhm_lecture.git), [pystan](https://pystan.readthedocs.io/en/latest/getting_started.html) and pickle to run (the last two are required solely for the final task).\n",
    "\n",
    "The model we're going to be inferring is below.\n",
    "\n",
    "<img src=\"bhm_plot.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "We start with imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# make sure everything we need is installed if running on Google Colab\n",
    "def is_colab():\n",
    "    try:\n",
    "        cfg = get_ipython().config\n",
    "        if cfg['IPKernelApp']['kernel_class'] == 'google.colab._kernel.Kernel':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except NameError:\n",
    "        return False\n",
    "if is_colab():\n",
    "    !pip install --quiet numpy matplotlib scipy corner pystan\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as mp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and immediately move to...\n",
    "\n",
    "## Task 2\n",
    "\n",
    "In which I ask you to write a Python function to generate a simulated Cepheid sample using the period-luminosity relation $m_{ij} = \\mu_i + M^* + s\\,\\log p_{ij} + \\epsilon(\\sigma_{\\rm int})$. For simplicity, assume Gaussian priors on everything, Gaussian intrinsic scatter and Gaussian measurement uncertainties. Assume only the first host has a distance modulus estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "n_gal = 2\n",
    "n_star = 200\n",
    "n_samples = 50000\n",
    "\n",
    "# PL relation parameters\n",
    "abs_bar = -26.0   # mean of standard absolute magnitude prior\n",
    "abs_sig = 4.0     # std dev of standard absolute magnitude prior\n",
    "s_bar = -1.0      # mean of slope prior\n",
    "s_sig = 1.0       # std dev of slope prior\n",
    "mu_bar = 30.0     # mean of distance modulus prior\n",
    "mu_sig = 5.0      # std dev of distance modulus prior\n",
    "m_sig_int = 0.05  # intrinsic scatter, assumed known\n",
    "\n",
    "# uncertainties\n",
    "mu_hat_sig = 0.01 # distance modulus measurement uncertainty\n",
    "m_hat_sig = 0.02  # apparent magnitude measurement uncertainty\n",
    "\n",
    "def simulate(n_gal, n_star, abs_bar, abs_sig, s_bar, s_sig, mu_bar, mu_sig, mu_hat_sig, m_sig_int, m_hat_sig):\n",
    "    \n",
    "    # TO BE COMPLETED\n",
    "    \n",
    "    # draw CPL parameters from Gaussian prior with means abs_bar and s_bar and standard deviations\n",
    "    # abs_sig and s_sig\n",
    "    abs_true = ??? + npr.randn() * ???\n",
    "    s_true = ??? + npr.randn() * ???\n",
    "    \n",
    "    # draw n_gal distance moduli from Gaussian prior with mean mu_bar and standard deviation mu_sig\n",
    "    # i've chosen to sort here so the closest galaxy is the one with the measured distance modulus\n",
    "    mu_true = np.sort(??? + npr.randn(n_gal) * ???)\n",
    "     \n",
    "    # measure ONLY ONE galaxy's distance modulus noisily. the noise here is assumed Gaussian with\n",
    "    # zero mean and standard deviation mu_hat_sig\n",
    "    mu_hat = ??? + npr.randn() * ???\n",
    "    \n",
    "    # draw log periods. these are assumed to be perfectly observed in this model, so they \n",
    "    # are simply a set of pre-specified numbers. i have chosen to generate new values with \n",
    "    # each simulation, drawn such that log-periods are uniformly drawn in the range 1-2 (i.e., \n",
    "    # 10 to 100 days). you can have these for free!\n",
    "    lp_true = 1.0 + npr.rand(n_gal, n_star)\n",
    "    \n",
    "    # draw true apparent magnitudes. these are distributed around the Cepheid period-luminosity \n",
    "    # relation with Gaussian intrinsic scatter (mean 0, standard deviation m_sig_int)\n",
    "    m_true = np.zeros((n_gal, n_star))\n",
    "    for i in range(n_gal):\n",
    "        m_true[i, :] = ??? + ??? + ??? * lp_true[i, :] + npr.randn(n_star) * ???\n",
    "    \n",
    "    # measure the apparent magnitudes noisily, all with the same measurement uncertainty m_hat_sig\n",
    "    m_hat = ??? + npr.randn(n_gal, n_star) * ???\n",
    "    \n",
    "    # return!\n",
    "    return (abs_true, s_true, mu_true, lp_true, m_true, mu_hat, m_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the simulation generates something sane. A simple test that the magnitude measurements errors are correctly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate\n",
    "abs_true, s_true, mu_true, lp_true, m_true, mu_hat, m_hat = \\\n",
    "    simulate(n_gal, n_star, abs_bar, abs_sig, s_bar, s_sig, mu_bar, mu_sig, mu_hat_sig, m_sig_int, m_hat_sig)\n",
    "\n",
    "# plot difference between true and observed apparent magnitudes. this should be the \n",
    "# noise, which is Gaussian distributed with mean zero and std dev m_hat_sig\n",
    "outs = mp.hist((m_true - m_hat).flatten())\n",
    "dm_grid = np.linspace(np.min(outs[1]), np.max(outs[1]))\n",
    "mp.plot(dm_grid, np.exp(-0.5 * (dm_grid/m_hat_sig) ** 2) * np.max(outs[0]))\n",
    "mp.xlabel(r'$m_{ij} - \\hat{m}_{ij}$')\n",
    "mp.ylabel(r'$N \\left(m_{ij} - \\hat{m}_{ij}\\right)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another test that the intrinsic scatter is added as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot difference between true apparent magnitudes and expected apparent \n",
    "# magnitude given a perfect (i.e., intrinsic-scatter-free) period-luminosity \n",
    "# relation. this should be the intrinsic scatter, which is Gaussian-\n",
    "# distributed with mean zero and std dev m_sig_int\n",
    "eps = np.zeros((n_gal, n_star))\n",
    "for i in range(n_gal):\n",
    "    eps[i, :] = mu_true[i] + abs_true + s_true * lp_true[i, :] - m_true[i, :]\n",
    "outs = mp.hist(eps.flatten())\n",
    "dm_grid = np.linspace(np.min(outs[1]), np.max(outs[1]))\n",
    "mp.plot(dm_grid, np.exp(-0.5 * (dm_grid/m_sig_int) ** 2) * np.max(outs[0]))\n",
    "mp.xlabel(r'$m_{ij} - \\hat{m}_{ij}$')\n",
    "mp.ylabel(r'$N \\left(m_{ij} - \\hat{m}_{ij}\\right)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Least Squares Demo\n",
    "\n",
    "Coding up the [GLS estimator](https://en.wikipedia.org/wiki/Generalized_least_squares) is a little involved, so I've done it for you below. Note that, rather unhelpfully, I've done so in a different order than in the notes. When I get a chance I will re-write. For now, you can simply evaluate the cells and bask in the glory of the fastest inference you will ever do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gls_fit(n_gal, n_star, mu_hat, mu_hat_sig, m_hat, m_sig_int, m_hat_sig, \\\n",
    "            lp_true, priors=None):\n",
    "\n",
    "    # setup\n",
    "    # n_obs is one anchor constraint and one magnitude per Cepheid.\n",
    "    # n_par is one mu per Cepheid host and 2 CPL params. if priors \n",
    "    # are used, we add on n_gal + 2 observations: one prior constraint \n",
    "    # on each host distance modulus and CPL parameter\n",
    "    n_obs = n_gal * n_star + 1\n",
    "    n_par = n_gal + 2\n",
    "    if priors is not None:\n",
    "        n_obs += n_gal + 2\n",
    "    data = np.zeros(n_obs)\n",
    "    design = np.zeros((n_obs, n_par))\n",
    "    cov_inv = np.zeros((n_obs, n_obs))\n",
    "    \n",
    "    # anchor\n",
    "    data[0] = mu_hat\n",
    "    design[0, 0] = 1.0\n",
    "    cov_inv[0, 0] = 1.0 / mu_hat_sig ** 2\n",
    "\n",
    "    # Cepheids\n",
    "    k = 1\n",
    "    for i in range(0, n_gal):\n",
    "        for j in range(0, n_star):\n",
    "\n",
    "            data[k] = m_hat[i, j]\n",
    "            design[k, i] = 1.0\n",
    "            design[k, n_gal] = 1.0\n",
    "            design[k, n_gal + 1] = lp_true[i, j]\n",
    "            cov_inv[k, k] = 1.0 / (m_hat_sig ** 2 + m_sig_int ** 2)\n",
    "            k += 1\n",
    "    \n",
    "    # and, finally, priors if desired\n",
    "    if priors is not None:\n",
    "        abs_bar, abs_sig, s_bar, s_sig, mu_bar, mu_sig = priors\n",
    "        for i in range(n_gal):\n",
    "            data[k] = mu_bar\n",
    "            design[k, i] = 1.0\n",
    "            cov_inv[k, k] = 1.0 / mu_sig ** 2\n",
    "            k += 1\n",
    "        data[k] = abs_bar\n",
    "        design[k, n_gal] = 1.0\n",
    "        cov_inv[k, k] = 1.0 / abs_sig ** 2\n",
    "        k += 1\n",
    "        data[k] = s_bar\n",
    "        design[k, n_gal + 1] = 1.0\n",
    "        cov_inv[k, k] = 1.0 / s_sig ** 2\n",
    "        k += 1\n",
    "        \n",
    "    # fit and return\n",
    "    destci = np.dot(design.transpose(), cov_inv)\n",
    "    pars_cov = np.linalg.inv(np.dot(destci, design))\n",
    "    pars = np.dot(np.dot(pars_cov, destci), data)\n",
    "    res = data - np.dot(design, pars)\n",
    "    dof = n_obs - n_par\n",
    "    chisq_dof = np.dot(res.transpose(), np.dot(cov_inv, res))\n",
    "    return pars, pars_cov, chisq_dof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gls_pars, gls_pars_cov, gls_chisq = gls_fit(n_gal, n_star, mu_hat, mu_hat_sig, m_hat, \\\n",
    "                                            m_sig_int, m_hat_sig, lp_true, \\\n",
    "                                            priors=[abs_bar, abs_sig, s_bar, s_sig, mu_bar, mu_sig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the outputs of the GLS fit we could draw a large number of samples from the resulting multivariate Gaussian posterior and pass them to something like [`corner`](https://corner.readthedocs.io/en/latest/); however, as we have analytic results we might as well use those directly. I've coded up something totally hacky here in order to do so. Information on how to draw confidence ellipses can be found in [Dan Coe's note](https://arxiv.org/pdf/0906.4123.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a hacky function designed to transform the analytic GLS outputs\n",
    "# into a corner.py style triangle plot, containing 1D and 2D marginalized\n",
    "# posteriors\n",
    "import scipy.stats as sps\n",
    "import matplotlib.patches as mpp\n",
    "def schmorner(par_mean, par_cov, par_true, par_label):\n",
    "    \n",
    "    # setup\n",
    "    par_std = np.sqrt(np.diag(par_cov))\n",
    "    x_min = par_mean[0] - 3.5 * par_std[0]\n",
    "    x_max = par_mean[0] + 3.5 * par_std[0]\n",
    "    y_min = par_mean[1] - 3.5 * par_std[1]\n",
    "    y_max = par_mean[1] + 3.5 * par_std[1]\n",
    "    fig, axes = mp.subplots(2, 2)\n",
    "    \n",
    "    # 1D marge\n",
    "    x = np.linspace(x_min, x_max, 100)\n",
    "    axes[0, 0].plot(x, sps.norm.pdf(x, par_mean[0], par_std[0]), 'k')\n",
    "    axes[0, 0].axvline(par_true[0])\n",
    "    axes[1, 0].axvline(par_true[0])\n",
    "    axes[0, 0].set_xticklabels([])\n",
    "    axes[0, 0].set_yticklabels([])\n",
    "    axes[0, 0].set_xlim(x_min, x_max)\n",
    "    axes[0, 0].set_title(par_label[0])\n",
    "    axes[0, 0].set_title(par_label[0] + r'$=' + '{:6.2f}'.format(par_mean[0]) + \\\n",
    "                         r'\\pm' + '{:4.2f}'.format(par_std[0]) + r'$')\n",
    "    y = np.linspace(y_min, y_max, 100)\n",
    "    axes[1, 1].plot(y, sps.norm.pdf(y, par_mean[1], par_std[1]), 'k')\n",
    "    axes[1, 0].axhline(par_true[1])\n",
    "    axes[1, 1].axvline(par_true[1])\n",
    "    axes[1, 1].tick_params(labelleft=False)\n",
    "    axes[1, 1].set_xlim(y_min, y_max)\n",
    "    for tick in axes[1, 1].get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    axes[1, 1].set_title(par_label[1] + r'$=' + '{:5.2f}'.format(par_mean[1]) + \\\n",
    "                         r'\\pm' + '{:4.2f}'.format(par_std[1]) + r'$')\n",
    "\n",
    "    # 2D marge\n",
    "    vals, vecs = np.linalg.eig(par_cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[::-1, 0]))\n",
    "    w, h = 2 * np.sqrt(vals)\n",
    "    ell = mpp.Ellipse(xy=par_mean, width=w, height=h,\n",
    "                      angle=theta, color='k')\n",
    "    ell.set_facecolor(\"none\")\n",
    "    axes[1, 0].add_artist(ell)\n",
    "    ell = mpp.Ellipse(xy=par_mean, width=2*w, height=2*h,\n",
    "                      angle=theta, color='k')\n",
    "    ell.set_facecolor(\"none\")\n",
    "    axes[1, 0].add_artist(ell)\n",
    "    axes[1, 0].set_xlim(x_min, x_max)\n",
    "    axes[1, 0].set_ylim(y_min, y_max)\n",
    "    for tick in axes[1, 0].get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    for tick in axes[1, 0].get_yticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    axes[1, 0].set_xlabel(par_label[0])\n",
    "    axes[1, 0].set_ylabel(par_label[1])\n",
    "    fig.delaxes(axes[0, 1])\n",
    "    fig.subplots_adjust(hspace=0, wspace=0)\n",
    "    \n",
    "test = schmorner(gls_pars[n_gal:], gls_pars_cov[n_gal:, n_gal:], \\\n",
    "                 [abs_true, s_true], [r'$M$', r'$s$'])\n",
    "#\n",
    "#lazy = npr.multivariate_normal(gls_pars[n_gal:], gls_pars_cov[n_gal:, n_gal:], n_samples)\n",
    "#fig = corner.corner(samples.T, labels=[r\"$M$\", r\"$s$\"],\n",
    "#                    show_titles=True, truths=[abs_bar, s_bar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3B\n",
    "\n",
    "Below I've written the majority of a Gibbs sampler to infer the hyper-parameters of the Cepheid PL relation from our simulated sample. One component is missing: drawing from the conditional distribution of the standard absolute magnitude, $M^*$. Please fill it in, using the results of whiteboard/paper Task 3A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sample(n_samples, n_gal, n_star, abs_bar, abs_sig, \\\n",
    "                 s_bar, s_sig, mu_bar, mu_sig, mu_hat_sig, \\\n",
    "                 m_sig_int, m_hat_sig, mu_hat, lp_true, m_hat):\n",
    "    \n",
    "    # storage\n",
    "    abs_samples = np.zeros(n_samples)\n",
    "    s_samples = np.zeros(n_samples)\n",
    "    mu_samples = np.zeros((n_gal, n_samples))\n",
    "    m_samples = np.zeros((n_gal, n_star, n_samples))\n",
    "    \n",
    "    # initialize sampler\n",
    "    abs_samples[0] = abs_bar + npr.randn() * abs_sig\n",
    "    s_samples[0] = s_bar + npr.randn() * s_sig\n",
    "    mu_samples[:, 0] = mu_bar + npr.randn(n_gal) * mu_bar\n",
    "    for i in range(n_gal):\n",
    "        m_samples[i, :, 0] = mu_samples[i, 0] + abs_samples[0] + s_samples[0] * lp_true[i, :]\n",
    "    \n",
    "    # sample!\n",
    "    for i in range(1, n_samples):\n",
    "        \n",
    "        # TO BE COMPLETED\n",
    "        # sample abs mag\n",
    "        \n",
    "        # sample slope\n",
    "        s_sig_pl = m_sig_int / np.sqrt(np.sum(lp_true ** 2))\n",
    "        s_bar_pl = 0.0\n",
    "        for j in range(n_gal):\n",
    "            s_bar_pl += np.sum((m_samples[j, :, i - 1] - mu_samples[j, i - 1] - abs_samples[i]) * lp_true[j, :])\n",
    "        s_bar_pl /= np.sum(lp_true ** 2)\n",
    "        s_std = np.sqrt((s_sig * s_sig_pl) ** 2 / (s_sig ** 2 + s_sig_pl ** 2))\n",
    "        s_mean = (s_sig ** 2 * s_bar_pl + s_sig_pl ** 2 * s_bar) / \\\n",
    "                 (s_sig ** 2 + s_sig_pl ** 2)\n",
    "        s_samples[i] = s_mean + npr.randn() * s_std\n",
    "        \n",
    "        # sample apparent magnitudes\n",
    "        for j in range(n_gal):\n",
    "            m_mean_pl = mu_samples[j, i - 1] + abs_samples[i] + s_samples[i] * lp_true[j, :]\n",
    "            m_std = np.sqrt(m_sig_int ** 2 * m_hat_sig ** 2 / (m_sig_int ** 2 + m_hat_sig ** 2))\n",
    "            m_mean = (m_sig_int ** 2 * m_hat[j, :] + m_hat_sig ** 2 * m_mean_pl) / (m_sig_int ** 2 + m_hat_sig ** 2)\n",
    "            m_samples[j, :, i] = m_mean + npr.randn(n_star) * m_std\n",
    "            \n",
    "        # sample distance moduli\n",
    "        mu_sig_pl = m_sig_int / np.sqrt(n_star)\n",
    "        mu_bar_pl = np.mean(m_samples[0, :, i] - abs_samples[i] - s_samples[i] * lp_true[0, :])\n",
    "        mu_var = 1.0 / (1.0 / mu_sig ** 2 + 1.0 / mu_hat_sig ** 2 + 1.0 / mu_sig_pl ** 2)\n",
    "        mu_mean = (mu_bar / mu_sig ** 2 + mu_hat / mu_hat_sig ** 2 + mu_bar_pl / mu_sig_pl ** 2) * mu_var\n",
    "        mu_samples[0, i] = mu_mean + npr.randn() * np.sqrt(mu_var)\n",
    "        for j in range(1, n_gal):\n",
    "            mu_sig_pl = m_sig_int / np.sqrt(n_star)\n",
    "            mu_bar_pl = np.mean(m_samples[j, :, i] - abs_samples[i] - s_samples[i] * lp_true[j, :])\n",
    "            mu_std = (mu_sig * mu_sig_pl) ** 2 / (mu_sig ** 2 + mu_sig_pl ** 2)\n",
    "            mu_mean = (mu_sig ** 2 * mu_bar_pl + mu_sig_pl ** 2 * mu_bar) / \\\n",
    "                      (mu_sig ** 2 + mu_sig_pl ** 2)\n",
    "            mu_samples[j, i] = mu_mean + npr.randn() * mu_std\n",
    "        \n",
    "    return (abs_samples, s_samples, mu_samples, m_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sample, setting aside the first half of the samples as warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = gibbs_sample(n_samples, n_gal, n_star, abs_bar, abs_sig, \\\n",
    "                           s_bar, s_sig, mu_bar, mu_sig, mu_hat_sig, \\\n",
    "                           m_sig_int, m_hat_sig, mu_hat, lp_true, m_hat)\n",
    "n_warmup = int(n_samples / 2)\n",
    "g_samples = [samples[n_warmup:] for samples in all_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the absolute magnitude is being inferred as expected. First, generate a trace plot of the absolute magnitude samples (the first entry in `g_samples`), overlaying the ground truth. Then print out the mean and standard deviation of the marginalized absolute magnitude posterior. Recall that marginalizing is as simple as throwing away the samples of all other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(g_samples[0])\n",
    "mp.axhline(abs_true)\n",
    "mp.xlabel('sample')\n",
    "mp.ylabel(r'$M^*$')\n",
    "print('Truth {:6.2f}; inferred {:6.2f} +/- {:4.2f}'.format(abs_true, np.mean(g_samples[0]), np.std(g_samples[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some marginalized parameter posteriors (by simply discarding all samples of the latent parameters) using DFM's [`corner`](https://corner.readthedocs.io/en/latest/) package. Note the near identical nature of this plot to the `schmorner` plot we generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "samples = np.stack((g_samples[0], g_samples[1]))\n",
    "fig = corner.corner(samples.T, labels=[r\"$M^*$\", r\"$s$\"],\n",
    "                    show_titles=True, truths=[abs_true, s_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "The final task is to write a [Stan model](https://pystan.readthedocs.io/en/latest/getting_started.html) to infer the parameters of the period-luminosity relation. I've coded up the other two blocks required (`data` and `parameters`), so all that is required is for you to write the joint posterior (factorized into its individual components) in Stan's sampling-statement-based syntax. Essentially all you need are Gaussian sampling statements (`abs_true ~ normal(abs_bar, abs_sig);`) and for loops (`for(i in 1: n_gal){...}`).\n",
    "\n",
    "When you evaluate this cell, Stan will translate your model into `c++` code and compile it. We will then pickle the compiled model so you can re-use it rapidly without recompiling. To do so, please set `recompile = False` in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pystan as ps\n",
    "import pickle\n",
    "\n",
    "stan_code = \"\"\"\n",
    "data {\n",
    "    int<lower=0> n_gal;\n",
    "    int<lower=0> n_star;\n",
    "    real mu_hat;\n",
    "    real mu_hat_sig;\n",
    "    real m_hat[n_gal, n_star];\n",
    "    real m_hat_sig;\n",
    "    real m_sig_int;\n",
    "    real lp_true[n_gal, n_star];\n",
    "    real abs_bar;\n",
    "    real abs_sig;\n",
    "    real s_bar;\n",
    "    real s_sig;\n",
    "    real mu_bar;\n",
    "    real mu_sig;\n",
    "}\n",
    "parameters {\n",
    "    real mu_true[n_gal];\n",
    "    real m_true[n_gal, n_star];\n",
    "    real abs_true;\n",
    "    real s_true;\n",
    "}\n",
    "model {\n",
    "    \n",
    "    // TO BE COMPLETED\n",
    "    // sample abs_true, s_true and mu_true from priors\n",
    "    // sample m_true[i, j] from PL relation with intrinsic scatter\n",
    "    // sample mu_hat and m_hat from relevant likelihoods\n",
    "    \n",
    "    // population-level priors\n",
    "    abs_true ~ normal(abs_bar, abs_sig);\n",
    "    s_true ~ normal(???, ???);\n",
    "    mu_true ~ normal(???, ???);\n",
    "    \n",
    "    // intrinsic scatter within CPL\n",
    "    for(i in 1: n_gal){\n",
    "        for(j in 1: n_star){\n",
    "            m_true[i, j] ~ normal(??? + ??? + ??? * lp_true[i, j], ???);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // likelihoods\n",
    "    mu_hat ~ normal(mu_true[1], mu_hat_sig);\n",
    "    for(i in 1: n_gal){\n",
    "        for(j in 1: n_star){\n",
    "            m_hat[i, j] ~ normal(???, ???);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "n_samples_stan = 5000\n",
    "recompile = True\n",
    "pkl_fname = 'bhms_stan_model_v{:d}p{:d}p{:d}.pkl'.format(sys.version_info[0], \\\n",
    "                                                         sys.version_info[1], \\\n",
    "                                                         sys.version_info[2])\n",
    "if recompile:\n",
    "    stan_model = ps.StanModel(model_code=stan_code)\n",
    "    with open(pkl_fname, 'wb') as f:\n",
    "        pickle.dump(stan_model, f)\n",
    "else:\n",
    "    try:\n",
    "        with open(pkl_fname, 'rb') as f:\n",
    "            stan_model = pickle.load(f)\n",
    "    except EnvironmentError:\n",
    "        print('ERROR: pickled Stan model (' + pkl_fname + ') not found. ' + \\\n",
    "              'Please set recompile = True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data = {'n_gal': n_gal, 'n_star': n_star, 'mu_hat': mu_hat, 'mu_hat_sig': mu_hat_sig, \\\n",
    "             'm_hat': m_hat, 'm_hat_sig': m_hat_sig, 'm_sig_int': m_sig_int, 'lp_true': lp_true, \\\n",
    "             'abs_bar': abs_bar, 'abs_sig': abs_sig, 's_bar': s_bar, 's_sig': s_sig, \\\n",
    "             'mu_bar': mu_bar, 'mu_sig': mu_sig}\n",
    "fit = stan_model.sampling(data=stan_data, iter=n_samples_stan, chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... print out Stan's posterior summary (note this is for _all_ parameters)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = fit.extract(permuted=True)\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the marginalized posterior of the PL parameters, as with the Gibbs sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_samples = np.stack((samples['abs_true'], samples['s_true']))\n",
    "fig = corner.corner(c_samples.T, labels=[r\"$M^*$\", r\"$s$\"],\n",
    "                    show_titles=True, truths=[abs_true, s_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our work here is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
